# ML Pipeline Workflow
# ====================
# End-to-end machine learning pipeline demonstrating:
# - Data preprocessing
# - Feature engineering
# - Model training with hyperparameter tuning
# - Evaluation and model selection
# - Deployment preparation
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ EXECUTION ENVIRONMENT                                                        │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Default: in-process (Python with scikit-learn/xgboost/lightgbm)             │
# │ Supported: in-process, subprocess, docker                                    │
# │                                                                              │
# │ Requirements:                                                                │
# │   - scikit-learn for preprocessing, RandomForest, evaluation                │
# │   - xgboost, lightgbm for gradient boosting                                 │
# │   - pytorch/tensorflow for neural networks (optional)                        │
# │   - optuna/hyperopt for hyperparameter tuning (optional)                    │
# │                                                                              │
# │ Docker execution:                                                            │
# │   - Image: victor-ml:latest                                                 │
# │   - Build: docker build -f docker/Dockerfile.ml -t victor-ml .              │
# │   - Useful for: GPU training, reproducibility, isolated environments        │
# │                                                                              │
# │ GPU considerations:                                                          │
# │   - For train_nn node, set executor=docker with nvidia runtime              │
# │   - XGBoost/LightGBM can use GPU with gpu_hist/gpu tree method              │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ DEFAULT VALUES                                                               │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ validation_score_threshold: 0.7 (data quality to proceed)                   │
# │ cv_folds: 5 (cross-validation folds)                                        │
# │ early_stopping_patience: 10 (epochs without improvement)                    │
# │ best_model_threshold: 0.85 (performance to auto-deploy)                     │
# │ model_output_format: pickle,onnx (serialization formats)                    │
# │ hitl_timeout: 1800s (30 min for low-score approval)                         │
# │ max_training_time: 900s (15 min per model)                                  │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ COMPUTE vs AGENT NODE RATIONALE                                              │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ COMPUTE nodes (no LLM reasoning):                                            │
# │   - load_train_data: File I/O (pd.read_csv)                                 │
# │   - validate_data: Boolean quality checks                                    │
# │   - numeric/categorical/text_features: Transformers from sklearn            │
# │   - feature_selection: MI, RFE, correlation filter algorithms               │
# │   - train_*: Model.fit() with GridSearchCV                                  │
# │   - evaluate_models: Metric computation (accuracy, F1, ROC-AUC)             │
# │   - save_model: pickle.dump(), onnx.export()                                │
# │   - create_api_spec: Template rendering                                      │
# │                                                                              │
# │ AGENT nodes (require LLM reasoning):                                         │
# │   - auto_fix_data: Deciding imputation strategy                             │
# │   - analyze_results: Interpreting metrics and trade-offs                    │
# │   - end_with_recommendations: Writing improvement suggestions               │
# │   - generate_docs: Writing model cards and documentation                    │
# └─────────────────────────────────────────────────────────────────────────────┘

workflows:
  ml_pipeline:
    description: "End-to-end ML pipeline with training and evaluation"

    metadata:
      version: "1.0"
      author: "victor"
      vertical: dataanalysis

    # =========================================================================
    # SERVICE DEFINITIONS
    # =========================================================================
    services:
      # Project database for experiment tracking
      project_db:
        type: sqlite
        config:
          path: $ctx.project_dir/.victor/project.db
          journal_mode: WAL
        lifecycle:
          start: auto
          cleanup: preserve

      # Model artifact storage
      model_storage:
        type: filesystem
        config:
          path: $ctx.model_output_dir
          auto_cleanup: false          # Preserve trained models
          max_size_gb: 20              # Models can be large
        lifecycle:
          start: auto
          cleanup: preserve            # Keep all artifacts

    batch_config:
      batch_size: 3
      max_concurrent: 2
      retry_strategy: immediate
      max_retries: 3

    nodes:
      # =====================================================================
      # Stage 1: Data Preparation
      # =====================================================================
      # COMPUTE: File I/O is deterministic parsing
      # ─────────────────────────────────────────────────────────────────────
      # Algorithm: pandas file readers (read_csv, read_parquet)
      # Loads train and test sets, validates target column exists
      #
      # WHY NOT AGENT: File parsing is mechanical - no judgment needed.
      # Schema validation is boolean (column exists or doesn't).
      #
      # Execution: in-process (pandas)
      # BLOCKED: [llm, network, write] - needs filesystem read
      - id: load_train_data
        type: compute
        name: "Load Training Data"
        tools: [read, shell]
        inputs:
          train_path: $ctx.train_file
          test_path: $ctx.test_file
          target_column: $ctx.target
        output: raw_datasets
        constraints: [llm, network, write]
        timeout: 60
        next: [validate_data]

      # COMPUTE: ML-specific data validation checks
      # ─────────────────────────────────────────────────────────────────────
      # Algorithm: Boolean checks for ML readiness:
      #   - check_target_present: target in df.columns
      #   - check_class_balance: np.bincount() / len for classification
      #   - check_missing_values: df.isna().sum() with threshold
      #   - check_feature_types: pd.api.types checks per column
      #
      # WHY NOT AGENT: All checks are True/False with clear criteria.
      # "Is there class imbalance?" = ratio > 10:1, not a judgment.
      #
      # Execution: in-process (pandas/numpy)
      # BLOCKED: [llm, filesystem, network, write] - pure in-memory
      - id: validate_data
        type: compute
        name: "Validate Data Quality"
        handler: data_transform
        inputs:
          data: $ctx.raw_datasets
          validations:
            - check_target_present
            - check_class_balance
            - check_missing_values
            - check_feature_types
        output: validation_report
        constraints: [llm, filesystem, network, write]
        timeout: 45
        next: [check_data_quality]

      - id: check_data_quality
        type: condition
        name: "Data Quality Gate"
        condition: "validation_score >= 0.7"
        branches:
          "true": feature_engineering
          "false": auto_fix_data

      - id: auto_fix_data
        type: agent
        name: "Auto-Fix Data Issues"
        role: executor
        goal: |
          Fix data quality issues automatically:
          - Impute missing values using appropriate strategies
          - Handle class imbalance if detected
          - Convert data types as needed
          - Remove or fix outliers
        tool_budget: 25
        llm_config:
          temperature: 0.1
          model_hint: claude-3-sonnet
        input_mapping:
          issues: validation_report
          data: raw_datasets
        output: fixed_datasets
        next: [feature_engineering]

      # =====================================================================
      # Stage 2: Feature Engineering
      # =====================================================================
      - id: feature_engineering
        type: parallel
        name: "Feature Engineering (Parallel)"
        parallel_nodes: [numeric_features, categorical_features, text_features]
        join_strategy: all
        next: [combine_features]

      # COMPUTE: Numeric feature transformation is sklearn transformers
      # ─────────────────────────────────────────────────────────────────────
      # Algorithm: sklearn.preprocessing transformers:
      #   - scaling: StandardScaler, MinMaxScaler, RobustScaler
      #   - polynomial_features: PolynomialFeatures(degree=2)
      #   - binning: KBinsDiscretizer
      # All are fit_transform() with deterministic output.
      #
      # WHY NOT AGENT: Transformations are mathematical formulas.
      # StandardScaler = (x - mean) / std - no judgment involved.
      #
      # Execution: in-process (scikit-learn)
      # BLOCKED: [llm, filesystem, network, write] - pure in-memory math
      - id: numeric_features
        type: compute
        name: "Process Numeric Features"
        tools: [shell]
        inputs:
          operations:
            - scaling
            - polynomial_features
            - binning
        output: numeric_transformed
        constraints: [llm, filesystem, network, write]
        timeout: 120

      # COMPUTE: Categorical encoding is lookup table construction
      # ─────────────────────────────────────────────────────────────────────
      # Algorithm: sklearn/category_encoders transformers:
      #   - one_hot_encoding: OneHotEncoder (sparse matrix)
      #   - target_encoding: TargetEncoder (mean target per category)
      #   - frequency_encoding: count/total per category
      #
      # WHY NOT AGENT: Encoding is mechanical mapping.
      # "red" -> [1,0,0], "blue" -> [0,1,0] - no interpretation needed.
      #
      # Execution: in-process (scikit-learn/category_encoders)
      # BLOCKED: [llm, filesystem, network, write] - pure computation
      - id: categorical_features
        type: compute
        name: "Process Categorical Features"
        tools: [shell]
        inputs:
          operations:
            - one_hot_encoding
            - target_encoding
            - frequency_encoding
        output: categorical_transformed
        constraints: [llm, filesystem, network, write]
        timeout: 120

      # COMPUTE: Text vectorization is algorithmic transformation
      # ─────────────────────────────────────────────────────────────────────
      # Algorithm:
      #   - tfidf: TfidfVectorizer (term freq * inverse doc freq)
      #   - word_embeddings: Pre-trained embeddings lookup (Word2Vec, FastText)
      #     OR sentence-transformers for contextual embeddings
      #
      # WHY NOT AGENT: TF-IDF is a formula. Embeddings are vector lookups.
      # Neither requires interpreting MEANING of text - that's done later.
      #
      # Execution: in-process (scikit-learn, gensim)
      #   For large corpora or transformer embeddings, consider subprocess
      #   to manage memory for embedding model loading
      #
      # BLOCKED: [llm, filesystem, write] - needs network for embedding API
      - id: text_features
        type: compute
        name: "Process Text Features"
        tools: [shell]
        inputs:
          operations:
            - tfidf
            - word_embeddings
        output: text_transformed
        constraints: [llm, filesystem, write]
        timeout: 180

      - id: combine_features
        type: transform
        name: "Combine All Features"
        transform: |
          feature_matrix = concat(
            numeric_transformed,
            categorical_transformed,
            text_transformed
          )
        next: [feature_selection]

      - id: feature_selection
        type: compute
        name: "Feature Selection"
        handler: parallel_tools
        tools: [shell]
        inputs:
          methods:
            - mutual_information
            - recursive_feature_elimination
            - correlation_filter
          top_k: 50
        output: selected_features
        constraints:
          llm_allowed: false
          timeout: 300
        next: [train_models]

      # =====================================================================
      # Stage 3: Model Training (Parallel)
      # =====================================================================
      - id: train_models
        type: parallel
        name: "Train Multiple Models"
        parallel_nodes: [train_rf, train_xgb, train_lgb, train_nn]
        join_strategy: all
        next: [evaluate_models]

      - id: train_rf
        type: compute
        name: "Train Random Forest"
        tools: [shell]
        inputs:
          model_type: random_forest
          hyperparams:
            n_estimators: [100, 200, 500]
            max_depth: [10, 20, null]
          cv_folds: 5
        output: rf_model
        constraints:
          llm_allowed: false
          timeout: 600

      - id: train_xgb
        type: compute
        name: "Train XGBoost"
        tools: [shell]
        inputs:
          model_type: xgboost
          hyperparams:
            n_estimators: [100, 300]
            learning_rate: [0.01, 0.1]
            max_depth: [6, 10]
          cv_folds: 5
        output: xgb_model
        constraints:
          llm_allowed: false
          timeout: 600

      - id: train_lgb
        type: compute
        name: "Train LightGBM"
        tools: [shell]
        inputs:
          model_type: lightgbm
          hyperparams:
            n_estimators: [100, 300]
            learning_rate: [0.01, 0.1]
            num_leaves: [31, 63]
          cv_folds: 5
        output: lgb_model
        constraints:
          llm_allowed: false
          timeout: 600

      - id: train_nn
        type: compute
        name: "Train Neural Network"
        tools: [shell]
        inputs:
          model_type: neural_network
          hyperparams:
            hidden_layers: [[64, 32], [128, 64, 32]]
            dropout: [0.2, 0.3]
          epochs: 100
          early_stopping: true
        output: nn_model
        constraints:
          llm_allowed: false
          timeout: 900

      # =====================================================================
      # Stage 4: Model Evaluation
      # =====================================================================
      - id: evaluate_models
        type: compute
        name: "Evaluate All Models"
        handler: parallel_tools
        tools: [shell]
        inputs:
          models:
            - $ctx.rf_model
            - $ctx.xgb_model
            - $ctx.lgb_model
            - $ctx.nn_model
          metrics:
            - accuracy
            - precision
            - recall
            - f1_score
            - roc_auc
        output: evaluation_results
        constraints:
          llm_allowed: false
          timeout: 300
        next: [analyze_results]

      - id: analyze_results
        type: agent
        name: "Analyze Model Performance"
        role: analyst
        goal: |
          Analyze model performance and provide recommendations:
          1. Compare all models across metrics
          2. Identify the best performing model
          3. Analyze trade-offs (speed vs accuracy)
          4. Check for overfitting signals
          5. Recommend the production model
        tool_budget: 15
        llm_config:
          temperature: 0.3
        input_mapping:
          results: evaluation_results
          validation: validation_report
        output: analysis
        next: [select_model]

      - id: select_model
        type: condition
        name: "Model Selection"
        condition: "best_model_score >= 0.85"
        branches:
          "true": prepare_deployment
          "false": request_approval

      - id: request_approval
        type: hitl
        name: "Low Score Approval"
        hitl_type: approval
        prompt: |
          ## Model Performance Below Threshold

          Best model achieved {best_model_score} (threshold: 0.85)

          **Analysis:**
          {analysis}

          Options:
          1. Proceed with current best model
          2. Request additional data/features
          3. Abort pipeline
        context_keys:
          - best_model_score
          - analysis
        choices:
          - "Proceed"
          - "Request more data"
          - "Abort"
        timeout: 1800
        fallback: abort
        next: [handle_approval]

      - id: handle_approval
        type: condition
        name: "Handle Approval Decision"
        condition: "approval_choice"
        branches:
          "Proceed": prepare_deployment
          "Request more data": end_with_recommendations
          "Abort": abort_pipeline

      - id: abort_pipeline
        type: transform
        name: "Abort Pipeline"
        transform: |
          pipeline_status = "aborted"
          abort_reason = "User decision due to low model performance"

      - id: end_with_recommendations
        type: agent
        name: "Generate Recommendations"
        role: analyst
        goal: |
          Generate recommendations for improving model performance:
          - Additional data sources needed
          - Feature engineering ideas
          - Alternative modeling approaches
        tool_budget: 10
        output: recommendations

      # =====================================================================
      # Stage 5: Deployment Preparation
      # =====================================================================
      - id: prepare_deployment
        type: parallel
        name: "Prepare for Deployment"
        parallel_nodes: [save_model, generate_docs, create_api_spec]
        join_strategy: all
        next: [final_review]

      - id: save_model
        type: compute
        name: "Save Best Model"
        tools: [shell, write]
        inputs:
          model: $ctx.best_model
          output_path: $ctx.model_output_dir
          format: "pickle,onnx"
        output: model_artifacts
        constraints:
          write_allowed: true
          timeout: 120

      - id: generate_docs
        type: agent
        name: "Generate Model Documentation"
        role: writer
        goal: |
          Generate model documentation including:
          - Model card (performance, limitations)
          - Feature descriptions
          - Usage examples
          - Deployment notes
        tool_budget: 15
        tools: [write]
        llm_config:
          temperature: 0.5
        output: documentation

      - id: create_api_spec
        type: compute
        name: "Create API Specification"
        tools: [shell, write]
        inputs:
          model_info: $ctx.best_model
          output_path: $ctx.api_spec_path
        output: api_spec
        constraints:
          write_allowed: true
          timeout: 60

      - id: final_review
        type: hitl
        name: "Final Deployment Review"
        hitl_type: approval
        prompt: |
          ## Model Ready for Deployment

          **Best Model:** {best_model_name}
          **Performance:** {best_model_score}

          **Artifacts Generated:**
          - Model: {model_artifacts}
          - Documentation: Generated
          - API Spec: {api_spec}

          Approve for deployment?
        context_keys:
          - best_model_name
          - best_model_score
          - model_artifacts
          - api_spec
        timeout: 600
        fallback: continue
        next: [complete]

      - id: complete
        type: transform
        name: "Pipeline Complete"
        transform: |
          pipeline_status = "completed"
          completion_time = current_timestamp()


  # =========================================================================
  # Quick Training - Baseline Model
  # =========================================================================
  ml_quick:
    description: "Quick ML training for baseline model"

    metadata:
      vertical: dataanalysis

    nodes:
      - id: load
        type: compute
        tools: [read, shell]
        output: data
        constraints:
          llm_allowed: false
          timeout: 30
        next: [quick_train]

      - id: quick_train
        type: compute
        name: "Train Quick Model"
        tools: [shell]
        inputs:
          model_type: random_forest
          hyperparams:
            n_estimators: 100
            max_depth: 10
        output: model
        constraints:
          llm_allowed: false
          timeout: 180
        next: [quick_eval]

      - id: quick_eval
        type: compute
        name: "Quick Evaluation"
        tools: [shell]
        output: metrics
        constraints:
          llm_allowed: false
          timeout: 60
        next: [summary]

      - id: summary
        type: agent
        name: "Quick Summary"
        role: analyst
        goal: "Provide a brief summary of baseline model performance."
        tool_budget: 5
        llm_config:
          temperature: 0.2
          model_hint: claude-3-haiku
        output: summary
